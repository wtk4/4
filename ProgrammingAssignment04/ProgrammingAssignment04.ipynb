{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e889f9-09e5-448f-8665-eedea26b19e4",
   "metadata": {},
   "source": [
    "# Programming Assignment #4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20069fa-1790-4d28-a662-3c4e2a39bba6",
   "metadata": {},
   "source": [
    "## 1. Linear Regression using scikit-learn\n",
    "\n",
    "The diamonds dataset contains the price, cut, color, and other characteristics of a sample of nearly 54,000 diamonds. This data can be used to predict the price of a diamond based on its characteristics. Use sklearn's LinearRegression() function to predict the price of a diamond from the diamond's carat and table values.\n",
    "\n",
    "- Import needed packages for regression.\n",
    "- Initialize and fit a multiple linear regression model.\n",
    "- Get the estimated intercept weight.\n",
    "- Get the estimated weights of the carat and table features.\n",
    "- Predict the price of a diamond with the user-input carat and table values.\n",
    "\n",
    "Ex: If the input is:\n",
    "\n",
    "- 0.5\n",
    "- 60\n",
    "\n",
    "the output should be:\n",
    "\n",
    "- Intercept is 1961.992\n",
    "- Weights for carat and table features are [7820.038  -74.301]\n",
    "- Predicted price is [1413.97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a53a2258-85ec-4f9c-9044-a3e03ea0f1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept is 1961.992\n",
      "Weights for carat and table features are [7820.038  -74.301]\n",
      "Predicted price is [925.47]\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages for regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Your code here\n",
    "\n",
    "# Silence warning from sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Input feature values for a sample instance\n",
    "carat = float(input(\"Enter carat value: \"))\n",
    "table = float(input(\"Enter table value: \"))\n",
    "\n",
    "# Load diamonds dataset\n",
    "diamonds = pd.read_csv('diamonds.csv')\n",
    "\n",
    "# Define input and output features\n",
    "X = diamonds[['carat', 'table']]\n",
    "y = diamonds['price']\n",
    "\n",
    "# Initialize a multiple linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the multiple linear regression model to the input and output features\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get estimated intercept weight\n",
    "intercept = model.intercept_\n",
    "print('Intercept is', round(intercept, 3))\n",
    "\n",
    "# Get estimated weights for carat and table features\n",
    "coefficients = model.coef_\n",
    "print('Weights for carat and table features are', np.round(coefficients, 3))\n",
    "\n",
    "# Predict the price of a diamond with the user-input carat and table values\n",
    "prediction = model.predict(np.array([[carat, table]]))\n",
    "print('Predicted price is', np.round(prediction, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa08d2a-995d-49b4-8ba2-01d7a58d8fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc74160c-0366-4a16-b131-effedf232ce5",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression using scikit-learn\n",
    "\n",
    "The **nbaallelo_log** file contains data on 126314 NBA games from 1947 to 2015. The dataset includes the features **pts, elo_i, win_equiv, and game_result**. Using the csv file **nbaallelo_log.csv** and scikit-learn's **LogisticRegression()** function, construct a logistic regression model to classify whether a team will win or lose a game based on the team's elo_i score.\n",
    "\n",
    "- Create a binary feature win for **game_result** with 0 for L and 1 for W\n",
    "- Use the **LogisticRegression()** function to construct a logistic regression model with **win** as the target and **elo_i** as the predictor\n",
    "- Print the weights and intercept of the fitted model\n",
    "- Find the proportion of instances correctly classified\n",
    "  \n",
    "Note: Use **ravel()** from **numpy** to flatten the second argument of **LogisticRegression.fit()** into a 1-D array.\n",
    "\n",
    "Ex: If the program uses the file **nbaallelo_small.csv**, which contains 100 instances, the output is:\n",
    "\n",
    "- w1: [[3.64194406e-06]]\n",
    "- w0: [-2.80257471e-09]\n",
    "- 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4045d6c1-6b4b-4cb8-b603-377bc6eb57bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1: [[0.00437785]]\n",
      "w0: [-6.54757518]\n",
      "0.599\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load nbaallelo_log.csv into a dataframe\n",
    "NBA = pd.read_csv('nbaallelo_log.csv')\n",
    "\n",
    "# Create binary feature for game_result with 0 for L and 1 for W\n",
    "NBA['win'] = NBA['game_result'].apply(lambda x: 1 if x == 'W' else 0)\n",
    "\n",
    "# Store relevant columns as variables\n",
    "X = NBA[['elo_i']]\n",
    "y = NBA[['win']].values.ravel()  # Flatten y to a 1-D array\n",
    "\n",
    "# Initialize and fit the logistic model using the LogisticRegression() function\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print the weights for the fitted model\n",
    "print('w1:', model.coef_)\n",
    "\n",
    "# Print the intercept of the fitted model\n",
    "print('w0:', model.intercept_)\n",
    "\n",
    "# Find the proportion of instances correctly classified\n",
    "score = model.score(X, y)\n",
    "print(round(score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d6f72-fff5-4ffd-b79d-cc89c2e6a5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8380de5-1123-4ae4-a35e-7067ae25bb61",
   "metadata": {},
   "source": [
    "## 3. Support Vector Classifier using scikit-learn\n",
    "\n",
    "The heart dataset contains 13 health-related attributes from 303 patients and one attribute denoting whether or not the patient has heart disease. Using the file heart.csv and scikit-learn's LinearSVC() function, fit a support vector classifier to predict whether a patient has heart disease based on other health attributes.\n",
    "\n",
    "- Import the correct packages and functions.\n",
    "- Split the data into 75% training data and 25% testing data. Set random_state=123.\n",
    "- Initialize and fit a support vector classifier with C=0.2, a maximum of 500 iterations, and random_state=123.\n",
    "- Print the model weights.\n",
    "\n",
    "Ex: If the program input is heart_small.csv, which contains 100 instances, the output is:\n",
    "\n",
    "0.6\n",
    "\n",
    "w0: [0.013]\n",
    "w1 and w2: [[ 0.361 -0.087]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08cfe80e-0a12-49fe-b251-5cc2d3defb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.671\n",
      "w0: [0.125]\n",
      "w1 and w2: [[ 0.39  -0.084]]\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the heart dataset\n",
    "heart = pd.read_csv('heart.csv')\n",
    "\n",
    "# Input features: thalach and age\n",
    "X = heart[['thalach', 'age']]\n",
    "\n",
    "# Output feature: target\n",
    "y = heart[['target']]\n",
    "\n",
    "# Create training and testing data with 75% training data and 25% testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "\n",
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize a support vector classifier with C=0.2 and a maximum of 500 iterations\n",
    "SVC = LinearSVC(C=0.2, max_iter=500, random_state=123)\n",
    "# Fit the support vector classifier according to the training data\n",
    "SVC.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate model on testing data\n",
    "score = SVC.score(X_test, y_test)\n",
    "print(np.round(score, 3))\n",
    "\n",
    "# Print the model weights\n",
    "# w0\n",
    "print('w0:', np.round(SVC.intercept_, 3))\n",
    "# w1 and w2\n",
    "print('w1 and w2:', np.round(SVC.coef_, 3))\n",
    "\n",
    "num_records_X_test = X_test.shape[0]\n",
    "print(num_records_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71d1f1-8716-44a9-bf75-577b55c3ca82",
   "metadata": {},
   "source": [
    "## 4. k-Nearest Neighbors using scikit-learn \n",
    "The dataset SDSS contains 17 observational features and one class feature for 10000 deep sky objects observed by the Sloan Digital Sky Survey. Use sklearn's KNeighborsClassifier() function to perform kNN classification to classify each object by the object's redshift and u-g color.\n",
    "\n",
    "- Import the necessary modules for kNN classification\n",
    "- Create dataframe X with features redshift and u_g\n",
    "- Create dataframe y with feature class\n",
    "- Initialize a kNN model with k=3\n",
    "- Fit the model using the training data\n",
    "- Find the predicted classes for the test data\n",
    "- Calculate the accuracy score using the test data\n",
    "\n",
    "Ex: If the feature u is used rather than u_g, the output is:\n",
    "- Accuracy score is 0.979"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e2ceea9-ca61-4001-b0fa-5b0ef4b85f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9901\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages for classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "skySurvey = pd.read_csv('SDSS.csv')\n",
    "\n",
    "# Create a new feature from u - g\n",
    "skySurvey['u_g'] = skySurvey['u'] - skySurvey['g']\n",
    "\n",
    "# Create dataframe X with features redshift and u_g\n",
    "X = skySurvey[['redshift', 'u_g']]\n",
    "\n",
    "# Create dataframe y with feature class\n",
    "y = skySurvey['class']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize model with k=3\n",
    "skySurveyKnn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit model using X_train and y_train\n",
    "skySurveyKnn.fit(X_train, y_train)\n",
    "\n",
    "# Find the predicted classes for X_test\n",
    "y_pred = skySurveyKnn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy score\n",
    "print('Accuracy score is ', end=\"\")\n",
    "print('%.3f' % score)\n",
    "\n",
    "shape_after_adding_u_g = skySurvey.shape\n",
    "print(shape_after_adding_u_g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2adb6-a4d7-4bcd-8777-2dd8e013d324",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes using scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493350b7-85b8-486b-b3c8-797425525e8d",
   "metadata": {},
   "source": [
    "The file SDSS contains 17 observational features and one class feature for 10000 deep sky objects observed by the Sloan Digital Sky Survey. Use sklearn's GaussianNB() function to perform Gaussian naive Bayes classification to classify each object by the object's redshift and u-g color.\n",
    "\n",
    "- Import the necessary modules for Gaussian naive Bayes classification\n",
    "- Create dataframe X with features redshift and u_g\n",
    "- Create dataframe y with feature class\n",
    "- Initialize a Gaussian naive Bayes model with the default parameters\n",
    "- Fit the model\n",
    "- Calculate the accuracy score\n",
    "\n",
    "Note: Use ravel() from numpy to flatten the second argument of GaussianNB.fit() into a 1-D array.\n",
    "\n",
    "Ex: If the feature u is used rather than u_g, the output is:\n",
    "\n",
    "- Accuracy score is 0.987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3604508a-dc4c-459b-9152-188228a5054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is 0.991\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "skySurvey = pd.read_csv('SDSS.csv')\n",
    "\n",
    "# Create a new feature from u - g\n",
    "skySurvey['u_g'] = skySurvey['u'] - skySurvey['g']\n",
    "\n",
    "# Create dataframe X with features redshift and u_g\n",
    "X = skySurvey[['redshift', 'u_g']]\n",
    "\n",
    "# Create dataframe y with feature class\n",
    "y = skySurvey['class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a Gaussian naive Bayes model\n",
    "skySurveyNBModel = GaussianNB()\n",
    "\n",
    "# Fit the model\n",
    "skySurveyNBModel.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Calculate the proportion of instances correctly classified\n",
    "score = skySurveyNBModel.score(X_test, y_test)\n",
    "\n",
    "# Print accuracy score\n",
    "print('Accuracy score is ', end=\"\")\n",
    "print('%.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47228886-e0e2-4536-914e-1315f59cab62",
   "metadata": {},
   "source": [
    "## 6. Ensemble methods using scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e30f1a-214f-4495-8468-74004e473a0e",
   "metadata": {},
   "source": [
    "## 6.1. Bagging using scikit-learn \n",
    "The msleep_clean dataset contains information on sleep habits for 47 mammals. Features include length of REM sleep, time spent awake, brain weight, and body weight.\n",
    "\n",
    "- Create a dataframe X containing the features awake, brainwt, and bodywt, in that order.\n",
    "- Create a dataframe y containing sleep_rem.\n",
    "- Initialize and fit a bagging regressor with 30 base estimators, a random state of 10, and oob_score=True.\n",
    "\n",
    "Ex: If 10 base estimators are used, the output should be:\n",
    "\n",
    "0.2322\n",
    "\n",
    "[3.26   2.92   1.0333 2.3333 0.8    1.325  2.56   2.2667 0.8    2.38\n",
    " 3.     0.5333 3.175  2.9667 0.7    0.65   1.825  2.2667 2.     1.\n",
    " 0.6    1.1667 1.5    3.1    2.     1.9    4.15   1.3    0.75   1.2\n",
    " 2.025  1.45   3.0286 2.72   0.5    2.0333 1.12   2.     2.65   1.65\n",
    " 2.6667 2.3    1.45   0.58   2.625  1.6    0.74   1.3   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5f86c9b-5dca-4e2c-bc0d-44bcac54b914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3144\n",
      "[3.1    2.98   0.8    1.6867 0.7167 1.8533 2.3091 2.0727 1.5231 2.1727\n",
      " 2.95   0.5375 2.9417 2.8727 0.9    0.7765 1.9818 2.5    1.8692 1.57\n",
      " 0.9692 1.1778 1.5769 2.75   2.4    2.1364 4.1727 1.6765 0.71   1.3909\n",
      " 2.13   1.9733 3.1429 3.3533 0.51   2.0077 1.4    2.0143 2.45   1.975\n",
      " 2.6    2.3    1.2    0.58   2.8222 1.9222 0.7417 1.24  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('msleep_clean.csv')\n",
    "\n",
    "# Create a dataframe X containing the features awake, brainwt, and bodywt, in that order\n",
    "X = df[['awake', 'brainwt', 'bodywt']]\n",
    "\n",
    "# Create a dataframe y containing sleep_rem\n",
    "y = df['sleep_rem']\n",
    "\n",
    "# Initialize and fit bagging regressor with 30 base estimators, a random state of 10, and oob_score=True\n",
    "sleepModel = BaggingRegressor(n_estimators=30, random_state=10, oob_score=True)\n",
    "sleepModel.fit(X, y)\n",
    "\n",
    "# Calculate out-of-bag accuracy\n",
    "print(np.round(sleepModel.oob_score_, 4))\n",
    "\n",
    "# Calculate predictions from out-of-bag estimate\n",
    "print(np.round(sleepModel.oob_prediction_, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60ca02-5530-41e4-a7c6-d5862b8554f6",
   "metadata": {},
   "source": [
    "## 6.2. Random forests using scikit-learn \n",
    "The mpg_clean.csv dataset contains information on miles per gallon (mpg) and engine size for cars sold from 1970 through 1982. Dataframe X contains the input features mpg, cylinders, displacement, horsepower, weight, acceleration, and model_year. Dataframe y contains the output feature origin.\n",
    "\n",
    "- Initialize and fit a random forest classifier with a user-input number of decision trees, estimator, a user-input number of features considered at each split, max_features, and a random state of 123.\n",
    "- Calculate the prediction accuracy for the model.\n",
    "- Read the documentation for the permutation_importance function from scikit-learn's inspection module.\n",
    "- Calculate the permutation importance using the default parameters and a random state of 123.\n",
    "\n",
    "Ex: When the input is\n",
    "\n",
    "5\n",
    "\n",
    "3\n",
    "\n",
    "the output is:\n",
    "\n",
    "0.9796\n",
    "\n",
    "\n",
    "     | Feature          | Permutation Importance |\n",
    "     |------------------|------------------------|\n",
    "    2| displacement     | 0.453571               |\n",
    "    0| mpg              | 0.160204               |\n",
    "    4| weight           | 0.133673               |\n",
    "    3| horsepower       | 0.107653               |\n",
    "    5| acceleration     | 0.057143               |\n",
    "    6| model_year       | 0.051531               |\n",
    "    1| cylinders        | 0.012245               |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c91ddba8-8bd1-42f4-ae22-9b5ed92ba0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8051\n",
      "        feature  permutation importance\n",
      "2  displacement                0.386441\n",
      "3    horsepower                0.115254\n",
      "4        weight                0.076271\n",
      "0           mpg                0.067797\n",
      "5  acceleration                0.042373\n",
      "6    model_year                0.010169\n",
      "1     cylinders                0.003390\n",
      "9\n",
      "        feature  permutation importance\n",
      "2  displacement                0.386441\n",
      "      feature  permutation importance\n",
      "3  horsepower                0.115254\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- acceleration\n- displacement\n- horsepower\n- model_year\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m horsepower_importance \u001b[38;5;241m=\u001b[39m importance_table[importance_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhorsepower\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(horsepower_importance)\n\u001b[1;32m---> 51\u001b[0m gradientBoostScore \u001b[38;5;241m=\u001b[39m \u001b[43mgradientBoostModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mround\u001b[39m(gradientBoostScore, \u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:764\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_gb.py:1612\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class for X.\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m \n\u001b[0;32m   1600\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1610\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m   1611\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1612\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1613\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# decision_function already squeezed it\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m         encoded_classes \u001b[38;5;241m=\u001b[39m (raw_predictions \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_gb.py:1565\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m   1547\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m \n\u001b[0;32m   1549\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;124;03m        array of shape (n_samples,).\u001b[39;00m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1568\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict(X)\n\u001b[0;32m   1569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- acceleration\n- displacement\n- horsepower\n- model_year\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('mpg_clean.csv')\n",
    "\n",
    "# Create a dataframe X containing the input features\n",
    "X = df.drop(columns=['name', 'origin'])\n",
    "\n",
    "# Create a dataframe y containing the output feature origin\n",
    "y = df['origin']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "# Get user-input n_estimators and max_features\n",
    "estimators = int(input())\n",
    "max_features = int(input())\n",
    "\n",
    "# Initialize and fit a random forest classifier with user-defined parameters and random_state=123\n",
    "rfModel = RandomForestClassifier(n_estimators=estimators, max_features=max_features, random_state=123)\n",
    "rfModel.fit(X_train, y_train)\n",
    "\n",
    "# Calculate prediction accuracy\n",
    "score = rfModel.score(X_test, y_test)\n",
    "print(round(score, 4))\n",
    "\n",
    "# Calculate the permutation importance using default parameters and random_state=123\n",
    "result = permutation_importance(rfModel, X_test, y_test, random_state=123)\n",
    "\n",
    "# Variable importance table\n",
    "importance_table = pd.DataFrame(\n",
    "    data={'feature': X.columns, 'permutation importance': result.importances_mean}\n",
    ").sort_values('permutation importance', ascending=False)\n",
    "\n",
    "print(importance_table)\n",
    "\n",
    "num_features = df.shape[1] \n",
    "print(num_features)\n",
    "\n",
    "displacement_importance = importance_table[importance_table['feature'] == 'displacement']\n",
    "print(displacement_importance)\n",
    "\n",
    "horsepower_importance = importance_table[importance_table['feature'] == 'horsepower']\n",
    "print(horsepower_importance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942139db-9cb7-410b-83c0-bacd96b7ebd4",
   "metadata": {},
   "source": [
    "## 6.3. Boosting using scikit-learn \n",
    "The mpg.csv dataset contains information on miles per gallon (mpg) and engine size for cars sold from 1970 through 1982.\n",
    "\n",
    "- Create a dataframe X containing the input features cylinders, weight, and mpg.\n",
    "- Create a dataframe y containing the output feature origin.\n",
    "- Initialize and fit an adaptive boosting classifier with a user-input learning rate lr and a random state of 123.\n",
    "- Initialize and fit a gradient boosting classifier with a user-input learning rate lr and a random state of 123.\n",
    "- Calculate the prediction accuracy for each model.\n",
    "\n",
    "Ex: If the user-input learning rate is 0.6, the output is:\n",
    "\n",
    "0.7688\n",
    "\n",
    "0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ccfd231-868f-4499-907a-1fb23106738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6417\n",
      "0.6417\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "mpg = pd.read_csv('mpg.csv')\n",
    "\n",
    "# Create a dataframe X containing the input features cylinders, weight, and mpg\n",
    "X = mpg[['cylinders', 'weight', 'mpg']]\n",
    "\n",
    "# Create a dataframe y containing the output feature origin\n",
    "y = mpg['origin']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "# Get user-input learning rate\n",
    "lr = float(input(\"Enter learning rate: \"))\n",
    "\n",
    "# Initialize and fit an adaptive boosting classifier with the user-input learning rate and a random state of 123\n",
    "adaBoostModel = AdaBoostClassifier(learning_rate=lr, random_state=123)\n",
    "adaBoostModel.fit(X_train, y_train)\n",
    "\n",
    "# Initialize and fit a gradient boosting classifier with the user-input learning rate and a random state of 123\n",
    "gradientBoostModel = GradientBoostingClassifier(learning_rate=lr, random_state=123)\n",
    "gradientBoostModel.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the prediction accuracy for the adaptive boosting classifier\n",
    "adaBoostScore = adaBoostModel.score(X_test, y_test)\n",
    "print(round(adaBoostScore, 4))\n",
    "\n",
    "# Calculate the prediction accuracy for the gradient boosting classifier\n",
    "gradientBoostScore = gradientBoostModel.score(X_test, y_test)\n",
    "print(round(gradientBoostScore, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59e220-2de2-4030-9abd-4bd3a7143a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
